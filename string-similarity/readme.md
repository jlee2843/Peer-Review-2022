This is a readme for just the string similarity functions and scripts (found in the string-similarity folder).

The main file to use is `main.ipynb`, which combines relevant functions into one jupyter notebook.

The `manual_pairs.py` file has a manually selected assortment of preprint-publication pairs, which are intended to be imported by another script (such as main).

A number of the string comparison functions are the same as or similar to those used in the 2016 paper "Comparing published scientific journal articles to their preprint versions" (republished 2018). In particular, length similarity, jaccard similarity, sorensen similarity, and levenshtein similarity should be the same metrics used in that paper. Cosine similarity should be similarity to the pairwise cosine similarity from that paper, albeit likely different in specific implementation details (we use sklearn's CountVectorizer).
Here, we're also including a comparison using cosine distance on the embeddings produced by a Hugging Face sentence transformer model (namely paraphrase-MiniLM-L6-v2).
We also use the natural language processing library spaCy for named entity recognition. From here we compare similarity based on shared numerics (hopefully providing a good indication of whether the results are the same) and shared named entities.
To see whether the sentiment between preprint and publication has changed, we use a Hugging Face Transformers sentiment analysis pipeline. After delimiting sentences using spaCy, the sentences' positive/negative sentiment is measured and the average over all the sentences is returned as the overall sentiment.
To see whether the readability of the text changes between preprint and publication, we use the Flesch Reading Ease and Flesch-Kincaid readability metrics implemented in the textstat library.
We also would potentially like to be able to perform a sort of sentence-by-sentence comparison. In order to do this, we create an algorithm (called match()) to match sentences from the preprint text with their best possible corresponding sentences in the publication version (sentences are again delimited using spaCy). We utilize a dynamic programming approach similar to longest-common-subsequence, but modified to be largest-weight-common-subsequence, where the weights are decided based on the similarity between any two given sentences. Sentence similarity is in turn measured similarly to some of the aforementioned methods.
The matching produced by the match() function is currently not yet being used to generate a metric, but the intention is that the gaps in the subsequence illustrate where additions and deletions were made; we can apply various metrics to the additions and deletions. We can also make comparisons along the common subsequence, likely producing different metrics which evaluate changes such as sentence structure.
The get_sca() function uses a library called neosca through command line, but is not currently recommended for use: it runs too slowly, has a number of dependencies, and we don't currently know how to use its results.